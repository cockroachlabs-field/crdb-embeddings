 The proposition in probability theory known as the "law of total expectation", |pages=380–383 }}  the "law of iterated expectations" Brilliant Math & Science Wiki|website=brilliant.org|language=en-us|access-date=2018-03-28}} ("LIE"), "Adam's law", the "tower rule",   and the "smoothing theorem", among other names, states that if <math>X</math> is a random variable whose expected value <math>\operatorname{E}(X)</math> is defined, and <math>Y</math> is any random variable on the same probability space, then :<math>\operatorname{E} (X) = \operatorname{E} ( \operatorname{E} ( X \mid Y)),</math> i.e., the expected value of the conditional expected value of <math>X</math> given <math>Y</math> is the same as the expected value of <math>X</math>. One special case states that if <math>{\left\{A_i\right\}}_i</math> is a finite or countable set|countable partition of a set|partition of the sample space, then :<math>\operatorname{E} (X) = \sum_i{\operatorname{E}(X \mid A_i) \operatorname{P}(A_i)}.</math> Note: The conditional expected value E("X" | "Z") is a random variable whose value depend on the value of "Z".  Note that the conditional expected value of "X" given the "event" "Z" = "z" is a function of "z".  If we write E("X" | "Z" = "z") = "g"("z") then the random variable E("X" | "Z") is "g"("Z"). Similar comments apply to the conditional covariance. ==Example== Suppose that only two factories supply light bulbs to the market. Factory <math>X</math><nowiki>'</nowiki>s bulbs work for an average of 5000 hours, whereas factory <math>Y</math><nowiki>'</nowiki>s bulbs work for an average of 4000 hours. It is known that factory <math>X</math> supplies 60% of the total bulbs available. What is the expected length of time that a purchased bulb will work for? Applying the law of total expectation, we have: : <math>\begin{align} \operatorname{E} (L) &= \operatorname{E}(L \mid X) \operatorname{P}(X)+\operatorname{E}(L \mid Y) \operatorname{P}(Y) \\ &= 5000(0.6)+4000(0.4)\\ &=4600 \end{align}</math> where * <math>\operatorname{E} (L)</math> is the expected life of the bulb; * <math>\operatorname{P}(X)={6 \over 10}</math> is the probability that the purchased bulb was manufactured by factory <math>X</math>; * <math>\operatorname{P}(Y)={4 \over 10}</math> is the probability that the purchased bulb was manufactured by factory <math>Y</math>; * <math>\operatorname{E}(L \mid X)=5000</math> is the expected lifetime of a bulb manufactured by <math>X</math>; * <math>\operatorname{E}(L \mid Y)=4000</math> is the expected lifetime of a bulb manufactured by <math>Y</math>. Thus each purchased light bulb has an expected lifetime of 4600 hours. ==Proof in the finite and countable cases== Let the random variables <math>X</math> and <math>Y</math>, defined on the same probability space, assume a finite or countably infinite set of finite values. Assume that <math>\operatorname{E}</math> is defined, i.e. <math>\min (\operatorname{E},  \operatorname{E}) < \infty</math>. If <math>\{A_i\}</math> is a partition of the probability space <math>\Omega</math>, then :<math>\operatorname{E} (X) = \sum_i{\operatorname{E}(X \mid A_i) \operatorname{P}(A_i)}.</math> "Proof." :<math> \begin{align} \operatorname{E} \left( \operatorname{E} (X \mid Y) \right) &= \operatorname{E} \Bigg \\ &=\sum_y \Bigg \cdot \operatorname{P}(Y=y) \\ &=\sum_y \sum_x x \cdot \operatorname{P}(X=x, Y=y). \end{align} </math> If the series is finite, then we can switch the summations around, and the previous expression will become :<math> \begin{align} \sum_x \sum_y x \cdot \operatorname{P}(X=x, Y=y)&=\sum_x x\sum_y \operatorname{P}(X=x, Y=y)\\ &=\sum_x x \cdot \operatorname{P}(X=x)\\ &=\operatorname{E}(X). \end{align} </math> If, on the other hand, the series is infinite, then its convergence cannot be Conditional convergence|conditional, due to the assumption that <math>\min (\operatorname{E}, \operatorname{E} ) < \infty.</math>  The series converges absolutely if both <math>\operatorname{E}</math> and <math>\operatorname{E}</math> are finite, and diverges to an infinity when either <math>\operatorname{E}</math> or <math>\operatorname{E}</math> is infinite.  In both scenarios, the above summations may be exchanged without affecting the sum. ==Proof in the general case== Let <math> (\Omega,\mathcal{F},\operatorname{P}) </math> be a probability space on which two sub Sigma-algebra|σ-algebras <math> \mathcal{G}_1 \subseteq \mathcal{G}_2 \subseteq \mathcal{F} </math> are defined. For a random variable <math> X </math> on such a space, the smoothing law states that if <math>\operatorname{E}</math> is defined, i.e. <math>\min(\operatorname{E}, \operatorname{E})<\infty</math>, then :<math> \operatorname{E} \mid \mathcal{G}_1] = \operatorname{E}\quad\text{(a.s.)}.</math> "Proof". Since a conditional expectation is a Radon–Nikodym theorem|Radon–Nikodym derivative, verifying the following two properties establishes the smoothing law: * <math> \operatorname{E} \mid \mathcal{G}_1] \mbox{ is } \mathcal{G}_1</math>-measurable * <math> \int_{G_1} \operatorname{E} \mid \mathcal{G}_1] d\operatorname{P} = \int_{G_1} X d\operatorname{P},</math> for all <math>G_1 \in \mathcal{G}_1.</math> The first of these properties holds by definition of the conditional expectation. To prove the second one, :<math> \begin{align} \min\left(\int_{G_1}X_+\, d\operatorname{P}, \int_{G_1}X_-\, d\operatorname{P}\right) &\leq \min\left(\int_\Omega X_+\, d\operatorname{P}, \int_\Omega X_-\, d\operatorname{P}\right)\\ &=\min(\operatorname{E}, \operatorname{E}) < \infty, \end{align} </math> so the integral <math>\textstyle \int_{G_1}X\, d\operatorname{P}</math> is defined (not equal <math>\infty - \infty</math>). The second property thus holds since <math>G_1 \in \mathcal{G}_1 \subseteq \mathcal{G}_2 </math> implies :<math>    \int_{G_1} \operatorname{E} \mid \mathcal{G}_1] d\operatorname{P} = \int_{G_1} \operatorname{E} d\operatorname{P} = \int_{G_1} X d\operatorname{P}. </math> "Corollary." In the special case when <math>\mathcal{G}_1 = \{\empty,\Omega \}</math> and <math>\mathcal{G}_2 = \sigma(Y)</math>, the smoothing law reduces to :<math>   \operatorname{E}. </math> "Alternative proof for <math> \operatorname{E}.</math>" This is a simple consequence of the measure-theoretic definition of conditional expectation.  By definition, <math> \operatorname{E} := \operatorname{E} </math> is a <math>\sigma(Y)</math>-measurable random variable that satisfies :<math>    \int_{A}\operatorname{E} d\operatorname{P} = \int_{A} X d\operatorname{P}, </math> for every measurable set <math> A \in \sigma(Y) </math>. Taking <math> A = \Omega </math> proves the claim. ==Proof of partition formula== :<math> \begin{align} \sum\limits_i\operatorname{E}(X\mid A_i)\operatorname{P}(A_i) &=\sum\limits_i\int\limits_\Omega X(\omega)\operatorname{P}(d\omega\mid A_i)\cdot\operatorname{P}(A_i)\\ &=\sum\limits_i\int\limits_\Omega X(\omega)\operatorname{P}(d\omega\cap A_i)\\ &=\sum\limits_i\int\limits_\Omega X(\omega)I_{A_i}(\omega)\operatorname{P}(d\omega)\\ &=\sum\limits_i\operatorname{E}(XI_{A_i}), \end{align} </math> where <math>I_{A_i}</math> is the indicator function of the set <math>A_i</math>. If the partition <math>{\{A_i\}}_{i=0}^n</math> is finite, then, by linearity, the previous expression becomes :<math> \operatorname{E}\left(\sum\limits_{i=0}^n XI_{A_i}\right)=\operatorname{E}(X), </math> and we are done. If, however, the partition <math>{\{A_i\}}_{i=0}^\infty</math> is infinite, then we use the dominated convergence theorem to show that :<math> \operatorname{E}\left(\sum\limits_{i=0}^n XI_{A_i}\right)\to\operatorname{E}(X). </math> Indeed, for every <math>n\geq 0</math>, :<math> \left|\sum_{i=0}^n XI_{A_i}\right|\leq |X|I_{\mathop{\bigcup}\limits_{i=0}^n A_i}\leq |X|. </math> Since every element of the set <math>\Omega</math> falls into a specific partition <math>A_i</math>, it is straightforward to verify that the sequence <math>{\left\{\sum_{i=0}^n XI_{A_i}\right\}}_{n=0}^\infty</math> pointwise convergence|converges pointwise to <math>X</math>. By initial assumption, <math>\operatorname{E}|X|<\infty</math>. Applying the dominated convergence theorem yields the desired result. ==See also== * The fundamental theorem of poker for one practical application. * Law of total probability * Law of total variance * Law of total covariance * Law of total cumulance * Product distribution#expectation (application of the Law for proving that the product expectation is the product of expectations) ==References==  * (Theorem 34.4) *Christopher Sims, , especially equations (16) through (18)  Category:Algebra of random variables Category:Theory of probability distributions Category:Statistical laws
