  File:Green's function animation.gif|alt=An animation that shows how Green's functions can be superposed to solve a differential equation subject to an arbitrary source.|thumb|360x360px|If one knows the solution <math display="inline">G(x,x')</math> to a differential equation subject to a point source <math display="inline">\hat{L}(x) G(x,x') = \delta(x-x')</math> and the differential operator <math display="inline">\hat{L}(x)</math> is linear, then one can superpose them to build the solution <math display="inline">u(x) = \int f(x') G(x,x') \, dx'</math> for a general source <math display="inline">\hat{L}(x) u(x) = f(x)</math>. In mathematics, a "Green's function" is the impulse response of an inhomogeneous ordinary differential equation|inhomogeneous linear differential operator defined on a domain with specified initial conditions or boundary conditions. This means that if <math>\operatorname{L}</math> is the linear differential operator, then * the Green's function <math>G</math> is the solution of the equation <math>\operatorname{L} G = \delta</math>, where <math>\delta</math> is Dirac delta function|Dirac's delta function; * the solution of the initial-value problem <math>\operatorname{L} y = f</math> is the convolution (<math>G \ast f</math>). Through the superposition principle, given a linear differential equation|linear ordinary differential equation (ODE), <math>\operatorname{L} y = f</math>, one can first solve <math>\operatorname{L} G = \delta_s</math>, for each , and realizing that, since the source is a sum of delta functions, the solution is a sum of Green's functions as well, by linearity of . Green's functions are named after the British mathematician George Green (mathematician)|George Green, who first developed the concept in the 1820s. In the modern study of linear partial differential equations, Green's functions are studied largely from the point of view of fundamental solutions instead. Under Green's function (many-body theory)|many-body theory, the term is also used in physics, specifically in quantum field theory, aerodynamics, aeroacoustics, electrodynamics, seismology and statistical field theory, to refer to various types of correlation function (quantum field theory)|correlation functions, even those that do not fit the mathematical definition. In quantum field theory, Green's functions take the roles of propagators. ==Definition and uses== A Green's function, , of a linear differential operator <math>\operatorname{L} = \operatorname{L}(x)</math> acting on distribution (mathematics)|distributions over a subset of the Euclidean space <math>\R^n</math>, at a point , is any solution of }} where  is the Dirac delta function. This property of a Green's function can be exploited to solve differential equations of the form }} If the kernel (linear operator)|kernel of  is non-trivial, then the Green's function is not unique. However, in practice, some combination of symmetry, boundary conditions and/or other externally imposed criteria will give a unique Green's function. Green's functions may be categorized, by the type of boundary conditions satisfied, by a Green's function number. Also, Green's functions in general are Distribution (mathematics)|distributions, not necessarily Function (mathematics)|functions of a real variable. Green's functions are also useful tools in solving wave equations and diffusion equations. In quantum mechanics, Green's function of the Hamiltonian mechanics|Hamiltonian is a key concept with important links to the concept of density of states. The Green's function as used in physics is usually defined with the opposite sign, instead. That is, <math display="block">\operatorname{L} \, G(x,s) = \delta(x-s)~.</math> This definition does not significantly change any of the properties of Green's function due to the evenness of the Dirac delta function. If the operator is translation invariant, that is, when <math>\operatorname{L}</math> has constant coefficients with respect to , then the Green's function can be taken to be a convolution kernel, that is, <math display="block">G(x,s) = G(x-s)~.</math> In this case, Green's function is the same as the impulse response of LTI system theory|linear time-invariant system theory. ==Motivation==  Loosely speaking, if such a function  can be found for the operator <math>\operatorname{L}</math>, then, if we multiply the equation&nbsp;() for the Green's function by , and then integrate with respect to , we obtain, <math display="block">\int \operatorname{L}\,G(x,s)\,f(s) \, ds = \int \delta(x-s) \, f(s) \, ds = f(x)~.</math> Because the operator <math>\operatorname{L} = \operatorname{L}(x)</math> is linear and acts only on the variable  (and "not" on the variable of integration ), one may take the operator <math>\operatorname{L}</math> outside of the integration, yielding <math display="block">\operatorname{L}\,\left(\int G(x,s)\,f(s) \,ds \right) = f(x)~.</math> This means that }} is a solution to the equation <math>\operatorname{L} u(x) = f(x)~.</math> Thus, one may obtain the function  through knowledge of the Green's function in equation&nbsp;() and the source term on the right-hand side in equation&nbsp;(). This process relies upon the linearity of the operator <math>\operatorname{L}</math>. In other words, the solution of equation&nbsp;(), , can be determined by the integration given in equation&nbsp;(). Although  is known, this integration cannot be performed unless  is also known. The problem now lies in finding the Green's function  that satisfies equation&nbsp;(). For this reason, the Green's function is also sometimes called the fundamental solution associated to the operator <math>\operatorname{L}</math>. Not every operator <math>\operatorname{L}</math> admits a Green's function. A Green's function can also be thought of as a Inverse function#Left and right inverses|right inverse of <math>\operatorname{L}</math>. Aside from the difficulties of finding a Green's function for a particular operator, the integral in equation&nbsp;() may be quite difficult to evaluate. However the method gives a theoretically exact result. This can be thought of as an expansion of  according to a Dirac delta function basis (projecting  over <math>\delta(x - s)</math>; and a superposition of the solution on each Projection (mathematics)|projection. Such an integral equation is known as a Fredholm integral equation, the study of which constitutes Fredholm theory.  ==Green's functions for solving inhomogeneous boundary value problems== The primary use of Green's functions in mathematics is to solve non-homogeneous boundary value problems. In modern theoretical physics, Green's functions are also usually used as propagators in Feynman diagrams; the term "Green's function" is often further used for any correlation function (quantum field theory)|correlation function.  Let <math>\operatorname{L}</math> be the Sturm–Liouville theory|Sturm–Liouville operator, a linear differential operator of the form <math display="block">\operatorname{L}=\dfrac{d}{dx}\left+q(x)</math> and let <math>\vec\operatorname{D}</math> be the vector-valued boundary conditions operator <math display="block">\vec\operatorname{D}\,u= \begin{bmatrix} \alpha_1 u'(0)+\beta_1 u(0) \\ \alpha_2 u'(\ell)+\beta_2 u(\ell) \end{bmatrix} ~.</math> Let <math>f(x)</math> be a continuous function in <math>\,.</math> Further suppose that the problem <math display="block">\begin{align}  \operatorname{L}\,u &= f \\  \vec\operatorname{D}\,u &= \vec{0} \end{align}</math> is "regular", i.e., the only solution for <math>f(x) = 0</math> for all  is <math>u(x) = 0</math>.  There is one and only one solution <math>u(x)</math> that satisfies <math display="block"> \begin{align}  \operatorname{L}\,u & = f\\  \vec\operatorname{D}\,u & = \vec{0} \end{align}</math> and it is given by <math display="block">u(x)=\int_0^\ell f(s) \, G(x,s) \, ds~,</math> where <math>G(x,s)</math> is a Green's function satisfying the following conditions: # <math>G(x,s)</math> is continuous in <math>x</math> and <math>s</math>. # For <math>x \ne s~</math>, <math>\quad \operatorname{L}\,G(x,s) = 0~</math>. # For <math>s \ne 0~</math>, <math>\quad \vec\operatorname{D}\,G(x,s) = \vec{0}~</math>. # Derivative "jump": <math>\quad G'(s_{0+},s) - G'(s_{0-},s) = 1 / p(s)~</math>. # Symmetry: <math>\quad G(x,s) = G(s,x)~</math>.   Green's function is not necessarily unique since the addition of any solution of the homogeneous equation to one Green's function results in another Green's function. Therefore if the homogeneous equation has nontrivial solutions, multiple Green's functions exist. In some cases, it is possible to find one Green's function that is nonvanishing only for <math>s \leq x</math>, which is called a retarded Green's function, and another Green's function that is nonvanishing only for <math>s \geq x </math>, which is called an advanced Green's function. In such cases, any linear combination of the two Green's functions is also a valid Green's function. The terminology advanced and retarded is especially useful when the variable x corresponds to time. In such cases, the solution provided by the use of the retarded Green's function depends only on the past sources and is causal whereas the solution provided by the use of the advanced Green's function depends only on the future sources and is acausal.  In these problems, it is often the case that the causal solution is the physically important one. The use of advanced and retarded Green's function is especially common for the analysis of solutions of the inhomogeneous electromagnetic wave equation. ==Finding Green's functions==  While it doesn't uniquely fix the form the Green's function will take, performing a dimensional analysis to find the units a Green's function must have is an important sanity check on any Green's function found through other means. A quick examination of the defining equation, <math display="block"> L G(x, s) = \delta(x - s), </math> shows that the units of <math>G</math> depend not only on the units of <math>L</math> but also on the number and units of the space of which the position vectors <math>x</math> and <math>s</math> are elements. This leads to the relationship: <math display="block"> G = L^{-1} d x^{-1}, </math> where <math>G</math> is defined as, "the physical units of <math>G</math>", and <math>d x</math> is the volume element of the space (or spacetime). For example, if <math>L = \partial_t^2</math> and time is the only variable then: <math display="block">L = \text{time}^{-2},</math> <math display="block">d x = \text{time},\ \text{and}</math> <math display="block">G = \text{time}.</math> If <math>L = \square = \frac{1}{c^2}\partial_t^2-\nabla^2</math>, the d'Alembert operator, and space has 3 dimensions then: <math display="block">L = \text{length}^{-2},</math> <math display="block">dx = \text{time} \text{length}^3,\ \text{and}</math> <math display="block">G = \text{time}^{-1} \text{length}^{-1}.</math>  If a differential operator  admits a set of eigenvectors  (i.e., a set of functions  and scalars  such that  ) that is complete, then it is possible to construct a Green's function from these eigenvectors and eigenvalues. "Complete" means that the set of functions }} satisfies the following completeness relation, <math display="block">\delta(x-x') = \sum_{n=0}^\infty \Psi_n^\dagger(x) \Psi_n(x').</math> Then the following holds,  where <math>\dagger</math> represents complex conjugation. Applying the operator  to each side of this equation results in the completeness relation, which was assumed. The general study of Green's function written in the above form, and its relationship to the function spaces formed by the eigenvectors, is known as Fredholm theory. There are several other methods for finding Green's functions, including the method of images, separation of variables, and Laplace transforms.(Cole 2011)  If the differential operator <math>L</math> can be factored as <math>L = L_1 L_2</math> then the Green's function of <math>L</math> can be constructed from the Green's functions for <math>L_1</math> and <math>L_2</math>: <math display="block"> G(x, s) = \int G_2(x, s_1)\, G_1(s_1, s) \, d s_1. </math> The above identity follows immediately from taking <math>G(x, s)</math> to be the representation of the right operator inverse of <math>L</math>, analogous to how for the Invertible matrix#Other properties|invertible linear operator <math>C</math>, defined by <math>C = (AB)^{-1} = B^{-1} A^{-1}</math>, is represented by its matrix elements <math>C_{i,j}</math>. A further identity follows for differential operators that are s ...
