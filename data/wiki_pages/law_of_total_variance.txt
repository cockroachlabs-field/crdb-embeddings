 In probability theory, the "law of total variance"Neil A. Weiss, "A Course in Probability", Addison&ndash;Wesley, 2005, pages 385&ndash;386. or "variance decomposition formula" or "conditional variance formulas" or "law of iterated variances" also known as "Eve's law", Joseph K. Blitzstein and Jessica Hwang: "Introduction to Probability" states that if <math>X</math> and <math>Y</math> are random variables on the same probability space, and the variance of <math>Y</math> is finite, then <math display=block> \operatorname{Var_Y}(Y) = \operatorname{E_X} + \operatorname{Var_X}(\operatorname{E_Y}). </math> In language perhaps better known to statisticians than to probability theorists, the two terms are the "unexplained" and the "explained" components of the variance respectively (cf. fraction of variance unexplained, explained variation). In actuarial science, specifically credibility theory, the first component is called the expected value of the process variance ("EVPV") and the second is called the variance of the hypothetical means ("VHM"). These two components are also the source of the term "Eve's law", from the initials EV VE for "expectation of variance" and "variance of expectation". ==Formulation== There is a general variance decomposition formula for <math>c \geq 2</math> components (see below).Bowsher, C.G. and P.S. Swain, Identifying sources of variation and the flow of information in biochemical networks, PNAS May 15, 2012 109 (20) E1320-E1328. For example, with two conditioning random variables: <math display=block>\operatorname{Var} = \operatorname{E}\left + \operatorname{E} \mid X_1)] + \operatorname{Var}(\operatorname{E}\left),</math> which follows from the law of total conditional variance: <math display=block>\operatorname{Var}(Y \mid X_1) = \operatorname{E} \left + \operatorname{Var} \left(\operatorname{E}\left \mid X_1\right).</math> Note that the conditional expected value <math>\operatorname{E}(Y \mid X)</math> is a random variable in its own right, whose value depends on the value of <math>X.</math> Notice that the conditional expected value of <math>Y</math> given the  <math>X = x</math> is a function of <math>x</math> (this is where adherence to the conventional and rigidly case-sensitive notation of probability theory becomes important!).  If we write <math>\operatorname{E}(Y \mid X = x) = g(x)</math> then the random variable <math>\operatorname{E}(Y \mid X)</math> is just <math>g(X).</math> Similar comments apply to the conditional variance. One special case, (similar to the law of total expectation) states that if <math>A_1, \ldots, A_n</math> is a partition of the whole outcome space, that is, these events are mutually exclusive and exhaustive, then <math display=block>\begin{align} \operatorname{Var} (X) = {} & \sum_{i=1}^n \operatorname{Var}(X\mid A_i) \Pr(A_i) + \sum_{i=1}^n \operatorname{E}^2 (1-\Pr(A_i))\Pr(A_i) \\ & {} - 2\sum_{i=2}^n \sum_{j=1}^{i-1} \operatorname{E} \Pr(A_i)\operatorname{E} \Pr(A_j). \end{align}</math> In this formula, the first component is the expectation of the conditional variance;  the other two components are the variance of the conditional expectation. ==Proof== The law of total variance can be proved using the law of total expectation.Neil A. Weiss, "A Course in Probability", Addison&ndash;Wesley, 2005, pages 380&ndash;383. First, <math display=block>\operatorname{Var} = \operatorname{E}\left - \operatorname{E}^2</math> from the definition of variance.  Again, from the definition of variance, and applying the law of total expectation, we have <math display=block>\operatorname{E}\left = \operatorname{E}\left\right] = \operatorname{E} \left + .</math> Now we rewrite the conditional second moment of <math>Y</math> in terms of its variance and first moment, and apply the law of total expectation on the right hand side: <math display=block>\operatorname{E}\left - \operatorname{E}^2 = \operatorname{E} \left +  - ^2.</math> Since the expectation of a sum is the sum of expectations, the terms can now be regrouped: <math display=block>= \left(\operatorname{E} ^2\right] - ^2\right).</math> Finally, we recognize the terms in the second set of parentheses as the variance of the conditional expectation <math>\operatorname{E}</math>: <math display=block>= \operatorname{E}  = {} & \operatorname{E}(\operatorname{Var}) \\ & {} + \sum_{j=2}^{c-1}\operatorname{E}(\operatorname{Var} \mid H_{1t},H_{2t},\ldots,H_{j-1,t}]) \\ & {} + \operatorname{Var}(\operatorname{E}). \end{align}</math> The decomposition is not unique. It depends on the order of the conditioning in the sequential decomposition. ==The square of the correlation and explained (or informational) variation== In cases where <math>(Y, X)</math> are such that the conditional expected value is linear; that is, in cases where <math display=block>\operatorname{E}(Y \mid X) = a X + b,</math> it follows from the bilinearity of covariance that   <math display=block>a={\operatorname{Cov}(Y, X) \over \operatorname{Var}(X)}</math> and <math display=block>b = \operatorname{E}(Y)-{\operatorname{Cov}(Y, X) \over \operatorname{Var}(X)} \operatorname{E}(X)</math> and the explained component of the variance divided by the total variance is just the square of the correlation between <math>Y</math> and <math>X;</math> that is, in such cases, <math display=block>{\operatorname{Var}(\operatorname{E}(Y \mid X)) \over \operatorname{Var}(Y)} = \operatorname{Corr}(X, Y)^2.</math> One example of this situation is when <math>(X, Y)</math> have a bivariate normal (Gaussian) distribution. More generally, when the conditional expectation <math>\operatorname{E}(Y \mid X)</math> is a non-linear function of <math>X</math> <math display=block>\iota_{Y\mid X} = {\operatorname{Var}(\operatorname{E}(Y \mid X)) \over \operatorname{Var}(Y)} = \operatorname{Corr}(\operatorname{E}(Y \mid X), Y)^2,</math> which can be estimated as the <math>R</math> squared from a non-linear regression of <math>Y</math> on <math>X,</math> using data drawn from the joint distribution of <math>(X, Y).</math> When <math>\operatorname{E}(Y \mid X)</math> has a Gaussian distribution (and is an invertible function of <math>X</math>), or <math>Y</math> itself has a (marginal) Gaussian distribution, this explained component of variation sets a lower bound on the mutual information: <math display=block>\operatorname{I}(Y; X) \geq \ln \left(^{-1/2}\right).</math> ==Higher moments== A similar law for the third central moment <math>\mu_3</math> says <math display=block>\mu_3(Y)=\operatorname{E}\left(\mu_3(Y \mid X)\right) + \mu_3(\operatorname{E}(Y \mid X)) + 3\operatorname{cov}(\operatorname{E}(Y \mid X), \operatorname{var}(Y \mid X)).</math> For higher cumulants, a generalization exists.  See law of total cumulance. ==See also== *  âˆ’ a generalization *  ==References==   *  *  (Problem 34.10(b)) <!-- *  --> Category:Algebra of random variables Category:Statistical deviation and dispersion Category:Articles containing proofs Category:Theory of probability distributions Category:Theorems in statistics Category:Statistical laws
