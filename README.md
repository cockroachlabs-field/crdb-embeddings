# CockroachDB: Store/Retrieve Based on Text Embeddings

## DISCLAIMER
This is a hack, an opportunistic exploitation of CockroachDB's trigram
support.  Since there is not yet any kind of official support for vector database-like
operations to efficiently handle the type of vectors representing text embeddings, this
was too irresistible.

The idea is take text embeddings generated by some model, sort them by the magnitude of
each array element, and chop that resulting array off to take only the top `N` values.
For each of these, a text token is generated based on the element's array index, using
a base 36 mapping (zero padded, 2 character string).  These are appended using a space
and this `token` is indexed using the GIN trigram index.  Lots of information is being
thrown away here, but it does preserve the most differentiating features, or that is the
idea at least.

## Future ideas

* Better testing against a pure Python + numpy setup which actually does the similarity scoring
via dot product
* Store either the entire vector, or just the portion being saved, in a `JSONB` column with
keys being the base36 encoded array index and values being the corresponding value from the
original array.  These would be used in scoring for rows which matched using the `%` operator.
* Put this into a Flask app to avoid the costly model initialization phase for each search
operation.

## Set up

```
$ export DB_URL="postgres://dbuser:passwd@127.0.0.1:26257/defaultdb?sslmode=require&sslrootcert=/crdb-certs/ca.crt"
$ psql $DB_URL
psql (16.1, server 13.0.0)
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_128_GCM_SHA256, compression: off)
Type "help" for help.

defaultdb=> DROP TABLE IF EXISTS text_embed;
DROP TABLE
defaultdb=> CREATE TABLE text_embed
defaultdb-> (
defaultdb(>   uri STRING NOT NULL
defaultdb(>   , chunk_num INT NOT NULL
defaultdb(>   , token STRING NOT NULL
defaultdb(>   , svec JSONB NOT NULL
defaultdb(>   , chunk STRING NOT NULL
defaultdb(>   , PRIMARY KEY (uri, chunk_num)
defaultdb(> );
CREATE TABLE
defaultdb=> CREATE INDEX ON text_embed USING GIN (token gin_trgm_ops);
CREATE INDEX

defaultdb=> CREATE OR REPLACE FUNCTION score_row (q JSONB, r JSONB)
defaultdb-> RETURNS FLOAT
defaultdb-> LANGUAGE SQL
defaultdb-> AS $$
defaultdb$>   SELECT COALESCE(SUM(qv * rv), 0.0) score
defaultdb$>   FROM (
defaultdb$>     SELECT
defaultdb$>       (json_each_text(q)).@1 qk
defaultdb$>       , ((json_each_text(q)).@2)::float qv
defaultdb$>       , (json_each_text(r)).@1 rk
defaultdb$>       , ((json_each_text(r)).@2)::float rv
defaultdb$>   )
defaultdb$>   WHERE qk = rk;
defaultdb$> $$;
CREATE FUNCTION
```

## Set up environment

```
$ vim env.sh
$ . ./env.sh
```

## Start the Flask server process

```
$ ./ngram_embeddings.py -s
pg_trgm.similarity_threshold: 0.2 (set via 'export MIN_SIM=0.1')
n_threads: 1 (set via 'export N_THREADS=10')
Log level: WARN (export LOG_LEVEL=[DEBUG|INFO|WARN|ERROR] to change this)
```

## Index some documents

```
$ . ./env.sh
$ time ./index_doc.py ./data/*.txt
data/5am_club.txt: SUCCESS (t = 1010.720 ms)
data/aus_nz.txt: SUCCESS (t = 323.854 ms)
data/bafta_awards.txt: SUCCESS (t = 179.528 ms)
data/bourdain.txt: SUCCESS (t = 322.123 ms)
[...]
data/top_jazz_albums.txt: SUCCESS (t = 429.001 ms)
data/undoctored.txt: SUCCESS (t = 491.508 ms)
data/victoria.txt: SUCCESS (t = 539.466 ms)

real	0m33.947s
user	0m0.135s
sys	0m0.048s
```

## Query the index

```
$ q="vacuum tube triode amplifier sound"
$ time ./search_client.sh $q
[
  {
    "uri": "data/decware_amp.txt",
    "sim": 0.318,
    "token": "69 1d in fs 3n hn f4 ez 7t jw a1 9f 95 ae k9 jo ke 9e 6p 0k fm 36 8d 8g 8u 5h bi 48 84 ii dk jv 6c 3o hk 38 8z j6 8i 4b 2x 35 bv fk k5 2f 8j 3j 1o fa 6v ji dw hj b2 81 a5 42 29 8b gd 62 9h 9o ",
    "chunk": "The result of this series vacuum tube power supply is a complete blocking of power supply harmonics, noise, hash, grain, and spikes"
  }
]

real	0m0.480s
user	0m0.010s
sys	0m0.016s
```

## How

See https://www.cockroachlabs.com/blog/use-cases-trigram-indexes/

Embeddings have some large dimensionality, like 768 or more.  Goal here is to
select the N most important dimensions, the ones with the largest magnitudes,
and generate a base36 token for each of these.

These will be concatenated with a space or, maybe, some other special
character between them and used as a token which will be indexed using a GIN with
the `gin_trgm_ops` option.

Likely, a second round of filtering using a regex type approach may boost relevance:
```
... AND chunk ~* '(termA|termB|...)'
```

The trigram index looks like this:
```
CREATE INDEX ON sentences USING GIN (embed_token gin_trgm_ops);
```

Python has [a base36 module](https://pypi.org/project/base36/)

```
$ pip install base36
```

Generate a zero-padded base36 string for a dimension in the embedding array:
```
import base36
base36.dumps(1).zfill(2)
```

Need to sort the dictionary of base36 key to float value from embedding array, by value.
Keep only the top n values.  Something like this:
```
n = 3
x = {"01": 0.123, "02": 0.922, "03": 0.456, "04": -0.999}
x_sorted = dict(sorted(x.items(), key=lambda item: abs(item[1]), reverse=True)[:n])
```

## References

* https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/
* https://huggingface.co/blog/bert-101
* https://huggingface.co/distilbert/distilbert-base-uncased
* https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/

